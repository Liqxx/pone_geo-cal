{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#icecube:\n",
    "from icecube import dataio, dataclasses, simclasses\n",
    "from icecube.icetray import OMKey\n",
    "from icecube.dataclasses import *\n",
    "\n",
    "# The usual:\n",
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "from scipy.interpolate import interp2d\n",
    "\n",
    "#Plotting:\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.style.use('/home/fhenningsen/plotformat.rc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using new GCD:\n",
    "geometry = dataio.I3File(\"/home/fhenningsen/gcd/physics_volume_GCD.i3.bz2\")\n",
    "gframe = geometry.pop_frame()  \n",
    "geo = gframe[\"I3Geometry\"] #access geo file via key\n",
    "\n",
    "#ceate a general event dictionary with 2D array (charge,time) as values\n",
    "event = {} \n",
    "all_dom_keys = []\n",
    "for i in geo.omgeo.keys():\n",
    "    if i.pmt==0 and i.string <87:\n",
    "        all_dom_keys.append(i)\n",
    "        \n",
    "for i in all_dom_keys:\n",
    "    event[i] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_string(abso, sca, domeff, p0, p1, Nph, N, pocam):\n",
    "    return 'ABS-%.3f_SCA-%.3f_DOME-%.3f_P0-%.3f_P1-%.3f_NPH-%.3e_N-%i_POCAM-%s' %(abso, sca,\n",
    "                                                                                  domeff, p0, p1,\n",
    "                                                                                  Nph, N,\n",
    "                                                                                  pocam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc = '/data/user/fhenningsen/deepcore_data/read_abs-sca-11/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get simulation set parameters\n",
    "params = np.load(os.path.join(direc, 'PARAMS.npy')).item()\n",
    "\n",
    "pocams     = params['pocams']\n",
    "pocam_keys = params['pocam_keys']\n",
    "truth_arr  = params['truth_arr']\n",
    "truth_str  = params['truth_string']\n",
    "\n",
    "# get dictionaries for values\n",
    "# data / scan\n",
    "scan_dict  = params['scan_dict']\n",
    "scan_N     = scan_dict['n']\n",
    "scan_Nph   = scan_dict['nph']\n",
    "scan_abs   = scan_dict['abs']\n",
    "scan_sca   = scan_dict['sca']\n",
    "scan_dome  = scan_dict['domeff']\n",
    "scan_p0    = scan_dict['p0']\n",
    "scan_p1    = scan_dict['p1']\n",
    "# truth\n",
    "truth_dict = params['truth_dict']\n",
    "t_N     = truth_dict['n']\n",
    "t_Nph   = truth_dict['nph']\n",
    "t_abs   = truth_dict['abs']\n",
    "t_sca   = truth_dict['sca']\n",
    "t_dome  = truth_dict['domeff']\n",
    "t_p0    = truth_dict['p0']\n",
    "t_p1    = truth_dict['p1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pocam_keys': [OMKey(87,84,0),\n",
       "  OMKey(88,72,0),\n",
       "  OMKey(89,38,0),\n",
       "  OMKey(90,100,0),\n",
       "  OMKey(91,50,0),\n",
       "  OMKey(92,28,0),\n",
       "  OMKey(93,64,0)],\n",
       " 'pocams': array(['87-84', '88-72', '89-38', '90-100', '91-50', '92-28', '93-64'],\n",
       "       dtype='|S6'),\n",
       " 'scan_dict': {'abs': array([0.9 , 0.92, 0.94, 0.96, 0.98, 1.  , 1.02, 1.04, 1.06, 1.08, 1.1 ]),\n",
       "  'domeff': array([1.]),\n",
       "  'n': array([100.]),\n",
       "  'nph': array([1.e+08]),\n",
       "  'p0': array([1.]),\n",
       "  'p1': array([0.]),\n",
       "  'sca': array([0.9 , 0.92, 0.94, 0.96, 0.98, 1.  , 1.02, 1.04, 1.06, 1.08, 1.1 ])},\n",
       " 'truth_arr': [array([1.02]),\n",
       "  array([1.02]),\n",
       "  array([1.]),\n",
       "  array([1.]),\n",
       "  array([0.]),\n",
       "  array([100.])],\n",
       " 'truth_dict': {'abs': array([1.02]),\n",
       "  'domeff': array([1.]),\n",
       "  'n': array([100.]),\n",
       "  'nph': array([1.e+08]),\n",
       "  'p0': array([1.]),\n",
       "  'p1': array([0.]),\n",
       "  'sca': array([1.02])},\n",
       " 'truth_string': 'TRUTH_ABS-1.020_SCA-1.020_DOME-1.000_P0-1.000_P1-0.000_N-100'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define bin size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using binsize of 40 ns\n"
     ]
    }
   ],
   "source": [
    "binsize = 40 # ns\n",
    "\n",
    "print 'Using binsize of', binsize, 'ns'\n",
    "\n",
    "n_bins_old = 5000\n",
    "n_bins_new = 5000/binsize\n",
    "time_bins  = np.linspace(0,n_bins_old,(n_bins_new+1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in truth and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_binned          = {}\n",
    "data_binned[\"truth\"] = {}\n",
    "data_binned[\"data\"]  = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth finished. Starting data...\n",
      "\n",
      "Parameter combo: 1.0 1.0 1.0 -0.7 -0.1 done!\n",
      "Parameter combo: 1.0 1.0 1.0 -0.7 -0.07 done!\n",
      "Parameter combo: 1.0 1.0 1.0 -0.7 -0.04 done!\n",
      "Parameter combo: 1.0 1.0 1.0 -0.7 -0.01 done!\n"
     ]
    }
   ],
   "source": [
    "### Read in truth ###\n",
    "for pocam in pocams:\n",
    "    par_t = param_string(t_abs, t_sca, t_dome, t_p0, t_p1, t_Nph, t_N, pocam)\n",
    "    data_tmp_1ns_binned = np.load(os.path.expanduser('%s/TRUTH_%s.npy'%(direc, par_t)))\n",
    "    data_binned[\"truth\"][pocam] = copy.deepcopy(event)\n",
    "    \n",
    "    for key in all_dom_keys:  #go through all DOMs\n",
    "        #rebinning by summing right number of 1ns bins to new binsize:\n",
    "        data_tmp_binned = [sum(data_tmp_1ns_binned.item().get(key)[i*binsize : (i+1)*binsize]) for i in range(n_bins_new)]\n",
    "        data_binned[\"truth\"][pocam][key]  = np.array(data_tmp_binned) / float(t_N / scan_N) # average to data size\n",
    "\n",
    "print \"Truth finished. Starting data...\\n\"\n",
    "    \n",
    "### Read in data ###\n",
    "for _abs in scan_abs:\n",
    "    for sca in scan_sca:\n",
    "        for dome in scan_dome:\n",
    "            for p0 in scan_p0:\n",
    "                for p1 in scan_p1:\n",
    "                    \n",
    "                    tag=\"{sca}_{dome}_{p0}_{p1}_{_abs}\".format(sca=sca,dome=dome,p0=p0,p1=p1,_abs=_abs)\n",
    "                    data_binned[\"data\"][tag] = {}\n",
    "                         \n",
    "                    for pocam in pocams: \n",
    "                        par = param_string(_abs, sca, dome, p0, p1, scan_Nph, scan_N, pocam)\n",
    "                        data_tmp_1ns_binned = np.load(os.path.expanduser('%s/%s.npy'%(direc, par)))\n",
    "                        data_binned[\"data\"][tag][pocam] = copy.deepcopy(event)\n",
    "\n",
    "                        for key in all_dom_keys:  # go through all DOMs\n",
    "                            # rebinning by summing right number of 1ns bins to new binsize:\n",
    "                            data_tmp_binned = [sum(data_tmp_1ns_binned.item().get(key)[i*binsize : (i+1)*binsize]) for i in range(n_bins_new)]\n",
    "                            data_binned[\"data\"][tag][pocam][key]  = data_tmp_binned\n",
    "                                \n",
    "                    print \"Parameter combo:\", _abs, sca, dome, p0, p1, \"done!\"\n",
    "print \"FINISHED!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = OMKey(81,50,0)\n",
    "\n",
    "y_hist=data_binned[\"truth\"][\"87-84\"][ok]\n",
    "plt.step(time_bins[:-1],y_hist,where='mid',alpha=0.9)\n",
    "plt.fill_between(time_bins[:-1],y_hist, step=\"mid\", alpha=0.3,label='truth')\n",
    "\n",
    "\n",
    "#compare with:\n",
    "#sca_dome_run:\n",
    "params=\"{sca}_{dome}_{p0}_{p1}_{_abs}\".format(sca=1.00,dome=1.00,p0=0.7 ,p1=0.2,_abs=1.00)\n",
    "y_hist=data_binned[\"data\"][params][\"87-84\"][ok]\n",
    "plt.step(time_bins[:-1],y_hist,where='mid',alpha=0.9)\n",
    "plt.fill_between(time_bins[:-1],y_hist, step=\"mid\", alpha=0.3,label=\"example data\")\n",
    "\n",
    "y_err=[np.sqrt(i/float(t_N/scan_N)) for i in y_hist]\n",
    "# plt.errorbar(x_time_2, y_hist_2, y_err,fmt='.k',elinewidth=1, capsize=1,markersize='3') \n",
    "\n",
    "plt.xlim(-100,2000)\n",
    "plt.ylim(0,None)\n",
    "plt.xlabel('Time [ns]', fontsize=14)\n",
    "plt.ylabel('Number of hits', fontsize=14)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('example_comp.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the LLH-landsape\n",
    "For the likelihhod we combine the likelihoods of individual DOMs of all POCAM flashes. To keep it managable and keep the individual count statistic high, we only look at DOMs that are within a 100m radius around the corresponing POCAM. \n",
    "<br>\n",
    "The likelihood is then:\n",
    "$$\n",
    "\\mathcal{L} = \\prod_{\\text{POCAMs}} \\, \\prod_{\\text{DOMs} < 100m} \\, \\prod_{\\text{histogram bins}} \\, p(d_i, t_i)\n",
    "$$\n",
    "and the log llh:\n",
    "$$\n",
    "-2 ln \\mathcal{L} = -2 \\sum_{\\text{POCAMs}} \\, \\sum_{\\text{DOMs}} \\, \\sum_{\\text{bins}} \\, ln( p(d_i, t_i))\n",
    "$$\n",
    "If $t_i$ > 20:\n",
    "$$\n",
    "p(d_i,t_i) = Normal(d_i,t_i) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\, e^{-\\frac{(d_i-t_i)^2}{2 \\sigma^2}} \\qquad \\text{where} \\quad \\sigma^2 = \\sqrt{t_i}^2 = t_i\n",
    "$$\n",
    "and if $t_i$ < 20:\n",
    "$$\n",
    "        p(d_i,t_i) = Poisson(d_i,t_i) = \\frac{t_i^{d_i}}{d_i !} \\, e^{-t_i} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data  = 40 # number of datapoints taken after first hits"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d = {}\n",
    "np.save('./llhdict_abs-sca.npy', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ab7fe3a701ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# check if bin/pts combination has been calculated before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mllhfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./llhdict_abs-sca.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mllhdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllhfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprofilekey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%i_%i'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import llh_main\n",
    "reload(llh_main)\n",
    "\n",
    "# check if bin/pts combination has been calculated before\n",
    "llhfile = './llhdict_abs-sca.npy'\n",
    "llhdict = np.load(llhfile).item()\n",
    "profilekey = '%i_%i' %(binsize, n_data)\n",
    "\n",
    "if profilekey in llhdict:\n",
    "    \n",
    "    print('LLH profile calculated before, using archived data.')\n",
    "    llh_array = llhdict[profilekey]\n",
    "\n",
    "else:\n",
    "\n",
    "    print('New LLH profile, calculating...')\n",
    "    \n",
    "    # Define empty llh-landscape:\n",
    "    llh_array=np.zeros((len(scan_sca), len(scan_abs)))\n",
    "\n",
    "    combo_cntr = 1\n",
    "    for sca_i,sca in enumerate(scan_sca):\n",
    "\n",
    "        for dome_i,dome in enumerate(scan_dome):\n",
    "\n",
    "            for p_i,p0 in enumerate(scan_p0):\n",
    "\n",
    "                for p_j,p1 in enumerate(scan_p1):\n",
    "\n",
    "                    for abs_i, _abs in enumerate(scan_abs):\n",
    "\n",
    "                        counter = 0\n",
    "\n",
    "                        # list of llh for pocams\n",
    "                        llh_pocam=[]\n",
    "\n",
    "                        for k,pocam in enumerate(pocams):\n",
    "\n",
    "                            # list of llh for doms\n",
    "                            llh_doms=[]\n",
    "\n",
    "                            # Get POCAM coordinates:\n",
    "                            pocam_key=pocam_keys[k]\n",
    "                            p_x=geo.omgeo[pocam_key].position.x\n",
    "                            p_y=geo.omgeo[pocam_key].position.y\n",
    "                            p_z=geo.omgeo[pocam_key].position.z\n",
    "\n",
    "                            hit_doms = 0\n",
    "                            for key in all_dom_keys:\n",
    "                                if key.pmt==0: # ignore upgrade receivers\n",
    "\n",
    "                                    # list of llh for datapoints\n",
    "                                    llh_datapoints=[]\n",
    "\n",
    "                                    # Get DOM coordinates:\n",
    "                                    d_x=geo.omgeo[key].position.x\n",
    "                                    d_y=geo.omgeo[key].position.y\n",
    "                                    d_z=geo.omgeo[key].position.z\n",
    "\n",
    "                                    distance= np.sqrt((d_x-p_x)**2 + (d_y-p_y)**2 + (d_z-p_z)**2)\n",
    "\n",
    "                                    # get data\n",
    "                                    par      = \"{sca}_{dome}_{p0}_{p1}_{_abs}\".format(sca=sca,dome=dome,\n",
    "                                                                                      p0=p0,p1=p1,_abs=_abs)\n",
    "                                    truth    = np.array(data_binned[\"truth\"][pocam][key])\n",
    "                                    data_sim = np.array(data_binned[\"data\"][par][pocam][key])\n",
    "\n",
    "                                    #print '%s %s \\t %.1f \\t %i %i' %(pocam, key, distance, sum(truth), sum(data_sim))\n",
    "\n",
    "                                    min_q     = 0\n",
    "                                    min_q_bin = 10\n",
    "\n",
    "                                    if sum(truth) > min_q and sum(data_sim) > min_q: \n",
    "\n",
    "                                        #print '%s %s \\t %.1f \\t %i' %(pocam, key, distance,sum(truth))\n",
    "                                        hit_doms += 1\n",
    "\n",
    "                                        index=next((i for i, x in enumerate(truth) if x), None) #gets first non-zero\n",
    "                                        truth    = truth[index:index+n_data]\n",
    "                                        data_sim = data_sim[index:index+n_data]\n",
    "\n",
    "                                        for i in range(n_data):\n",
    "                                            t = float(truth[i])\n",
    "                                            d = float(data_sim[i])\n",
    "\n",
    "                                            # remove non-hit doms (and apply some minimum bin value)\n",
    "                                            if t > 0 and t > min_q_bin:\n",
    "\n",
    "                                                # get global llh definition\n",
    "                                                llh = llh_main.llh_grid(d,t, chi2=True, chi2modunc=True)\n",
    "                                                # append it \n",
    "                                                llh_datapoints.append(llh)\n",
    "\n",
    "\n",
    "                                        # Append llh sum from DOM:\n",
    "                                        counter += 1\n",
    "                                        llh_doms.append(sum(llh_datapoints)) \n",
    "\n",
    "                            # Append llh sum from POCAM:\n",
    "                            llh_pocam.append(sum(llh_doms) / float(hit_doms)) \n",
    "\n",
    "                        # Putting the final llh into the landscape array:\n",
    "                        llh_array[sca_i,abs_i] = sum(llh_pocam) / float(len(pocams))\n",
    "                        print('%i / %i combinations done.' %(combo_cntr, int(len(scan_sca) * len(scan_abs))))\n",
    "                        combo_cntr += 1\n",
    "    \n",
    "    # store new profile \n",
    "    llhdict[profilekey] = llh_array\n",
    "    np.save(llhfile, llhdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# likelihood profile\n",
    "x  = scan_p0\n",
    "y  = scan_p1\n",
    "z  = llh_array\n",
    "dx = np.abs((np.unique(x)[1:] - np.unique(x)[:-1])/2)[0]\n",
    "dy = np.abs((np.unique(y)[1:] - np.unique(y)[:-1])/2)[0]\n",
    "xx = np.unique(np.append(x - dx, sorted(x + dx)[-1]))\n",
    "yy = np.unique(np.append(y - dy, sorted(y + dy)[-1]))\n",
    "im = ax.pcolormesh(xx, yy, z)\n",
    "\n",
    "# colorbar\n",
    "cbar = plt.colorbar(im, orientation='vertical',fraction=0.045,ax=ax)\n",
    "cbar.set_label(r'$-2 \\, \\frac{ln \\, \\mathcal{L}}{N}$',fontsize=18)\n",
    "\n",
    "# truth\n",
    "tx = t_p0\n",
    "ty = t_p1\n",
    "plt.scatter(tx, ty, color='r',s=150,label='MC truth',marker='*')\n",
    "\n",
    "# llh min\n",
    "index_1,index_2=np.where(llh_array==np.amin(llh_array))\n",
    "ly = scan_p1[int(index_1)]\n",
    "lx = scan_p0[int(index_2)]\n",
    "plt.scatter(lx, ly, color='cyan',s=250,label='LLH min',marker='*')\n",
    "\n",
    "### llh intervals ###\n",
    "# index_s1,index_s2=np.where(llh_array<np.amin(llh_array)+2.7)\n",
    "# plt.scatter(index_s2,index_s1,label=\"$< llh_{min} + 2.7$\",c='yellow',s=100,alpha=0.8)\n",
    "\n",
    "# formatting\n",
    "ax.set_xlabel('P0')\n",
    "ax.set_ylabel('P1')\n",
    "ax.legend(fontsize=13)\n",
    "plt.title(\"Grid search with {_bin} ns bins / {pts} pts\".format(_bin=binsize, pts=n_data),fontsize=14)\n",
    "plt.savefig('./grid-search-p0-p1_binning-%ins-%ipts.pdf' %(binsize, n_data))\n",
    "plt.show()\n",
    "\n",
    "print \"LLH minimum at p0, p1:\", (lx, ly)\n",
    "print \"Truth at                  :\", (tx, ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scan_p0\n",
    "y = scan_p1\n",
    "z = llh_array\n",
    "n = 100\n",
    "\n",
    "f    = interp2d(np.unique(x), np.unique(y), z, kind='cubic')\n",
    "xn   = np.linspace(x.min(), x.max(), n)\n",
    "yn   = np.linspace(y.min(), y.max(), n)\n",
    "zzz  = f(xn, yn)\n",
    "dxn  = np.abs((np.unique(xn)[1:] - np.unique(xn)[:-1])/2)[0]\n",
    "dyn  = np.abs((np.unique(yn)[1:] - np.unique(yn)[:-1])/2)[0]\n",
    "xxn  = np.unique(np.append(xn - dxn, sorted(xn + dxn)[-1]))\n",
    "yyn  = np.unique(np.append(yn - dyn, sorted(yn + dyn)[-1]))\n",
    "xxx, yyy = np.meshgrid(xxn, yyn)\n",
    "xxxm, yyym = np.meshgrid(xn, yn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# interpolated llh profile\n",
    "im = ax.pcolormesh(xxx,yyy,zzz)\n",
    "cb = plt.colorbar(im, orientation='vertical',\n",
    "                  fraction=0.05,ax=ax, label=r'$-2 \\, \\frac{ln \\, \\mathcal{L}}{N}$')\n",
    "\n",
    "# truth and min\n",
    "tx  = t_p0\n",
    "ty  = t_p1\n",
    "idx = np.where(zzz==np.amin(zzz))\n",
    "lx  = xxx[idx]\n",
    "ly  = yyy[idx]\n",
    "plt.scatter(tx, ty, color='r',s=150,label='MC truth',marker='*')\n",
    "plt.scatter(lx, ly, color='cyan',s=150,label='LLH min',marker='*')\n",
    "\n",
    "# contours\n",
    "zmin = np.min(zzz)    \n",
    "cs = ax.contour(xxxm, yyym, zzz, levels=zmin + 2.7 * np.array([1,2,3]), \n",
    "                colors=['white'], linestyles=['--'], label='Confidence level')\n",
    "fmt  = {}\n",
    "strs = [r'$1\\sigma$', r'$2\\sigma$', r'$3\\sigma$']\n",
    "for l, s in zip(cs.levels, strs):\n",
    "    fmt[l] = s\n",
    "ax.clabel(cs, cs.levels, inline=True, fmt=fmt)\n",
    "\n",
    "# legend\n",
    "hs, ls = ax.get_legend_handles_labels()\n",
    "h,_    = cs.legend_elements()\n",
    "plt.legend(hs+h, ls+['Confidence level'])\n",
    "plt.title(\"Grid search with {_bin} ns bins / {pts} pts\".format(_bin=binsize, pts=n_data),fontsize=14)\n",
    "plt.xlabel('P0')\n",
    "plt.ylabel('P1')\n",
    "plt.savefig('./grid-search-p0-p1_binning-%ins-%ipts_interp.pdf' %(binsize, n_data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
