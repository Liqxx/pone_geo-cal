{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# icecube \n",
    "from icecube.dataclasses import *\n",
    "from icecube.icetray import OMKey\n",
    "from icecube import dataio, dataclasses, simclasses\n",
    "\n",
    "# usual\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp2d\n",
    "\n",
    "# plotting\n",
    "%matplotlib notebook\n",
    "from matplotlib import rcParams\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.style.use('/home/fhenningsen/plotformat.rc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using new GCD\n",
    "geometry = dataio.I3File(\"/home/fhenningsen/gcd/physics_volume_GCD.i3.bz2\")\n",
    "gframe = geometry.pop_frame()  \n",
    "geo = gframe[\"I3Geometry\"] #access geo file via key\n",
    "\n",
    "#ceate a general event dictionary with 2D array (charge,time) as values\n",
    "event = {} \n",
    "all_dom_keys = []\n",
    "for i in geo.omgeo.keys():\n",
    "    if i.pmt == 0 and i.string < 87 and i.om <= 60:\n",
    "        all_dom_keys.append(i)\n",
    "        \n",
    "for i in all_dom_keys:\n",
    "    event[i] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_string(abso, sca, domeff, p0, p1, Nph, N, pocam):\n",
    "    return 'ABS-%.3f_SCA-%.3f_DOME-%.3f_P0-%.3f_P1-%.3f_NPH-%.3e_N-%i_POCAM-%s' %(abso, sca,\n",
    "                                                                                  domeff, p0, p1,\n",
    "                                                                                  Nph, N,\n",
    "                                                                                  pocam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc = '/data/user/fhenningsen/deepcore_data/read-3d-domeff-p0-p1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/data/user/fhenningsen/deepcore_data/read-3d-domeff-p0-p1/PARAMS.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-df45b25fc448>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get simulation set parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PARAMS.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpocams\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pocams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpocam_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pocam_keys'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/icecube.opensciencegrid.org/py2-v3.1.1/RHEL_7_x86_64/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/data/user/fhenningsen/deepcore_data/read-3d-domeff-p0-p1/PARAMS.npy'"
     ]
    }
   ],
   "source": [
    "# get simulation set parameters\n",
    "params = np.load(os.path.join(direc, 'PARAMS.npy')).item()\n",
    "\n",
    "pocams     = params['pocams']\n",
    "pocam_keys = params['pocam_keys']\n",
    "truth_arr  = params['truth_arr']\n",
    "truth_str  = params['truth_string']\n",
    "\n",
    "# get dictionaries for values\n",
    "# data / scan\n",
    "scan_dict  = params['scan_dict']\n",
    "scan_N     = scan_dict['n']\n",
    "scan_Nph   = scan_dict['nph']\n",
    "scan_abs   = scan_dict['abs']\n",
    "scan_sca   = scan_dict['sca']\n",
    "scan_dome  = scan_dict['domeff']\n",
    "scan_p0    = scan_dict['p0']\n",
    "scan_p1    = scan_dict['p1']\n",
    "# truth\n",
    "truth_dict = params['truth_dict']\n",
    "t_N     = truth_dict['n']\n",
    "t_Nph   = truth_dict['nph']\n",
    "t_abs   = truth_dict['abs']\n",
    "t_sca   = truth_dict['sca']\n",
    "t_dome  = truth_dict['domeff']\n",
    "t_p0    = truth_dict['p0']\n",
    "t_p1    = truth_dict['p1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pocam_keys': [OMKey(87,84,0),\n",
       "  OMKey(88,72,0),\n",
       "  OMKey(89,38,0),\n",
       "  OMKey(90,100,0),\n",
       "  OMKey(91,50,0),\n",
       "  OMKey(92,28,0),\n",
       "  OMKey(93,64,0)],\n",
       " 'pocams': array(['87-84', '88-72', '89-38', '90-100', '91-50', '92-28', '93-64'],\n",
       "       dtype='|S6'),\n",
       " 'scan_dict': {'abs': array([1.]),\n",
       "  'domeff': array([0.9 , 0.92, 0.94, 0.96, 0.98, 1.  , 1.02, 1.04, 1.06, 1.08, 1.1 ]),\n",
       "  'n': array([100.]),\n",
       "  'nph': array([1.e+08]),\n",
       "  'p0': array([1.]),\n",
       "  'p1': array([0.]),\n",
       "  'sca': array([0.9 , 0.92, 0.94, 0.96, 0.98, 1.  , 1.02, 1.04, 1.06, 1.08, 1.1 ])},\n",
       " 'truth_arr': [array([1.]),\n",
       "  array([1.03]),\n",
       "  array([0.96]),\n",
       "  array([1.]),\n",
       "  array([0.]),\n",
       "  array([100.])],\n",
       " 'truth_dict': {'abs': array([1.]),\n",
       "  'domeff': array([0.96]),\n",
       "  'n': array([100.]),\n",
       "  'nph': array([1.e+08]),\n",
       "  'p0': array([1.]),\n",
       "  'p1': array([0.]),\n",
       "  'sca': array([1.03])},\n",
       " 'truth_string': 'TRUTH_ABS-1.000_SCA-1.030_DOME-0.960_P0-1.000_P1-0.000_N-100'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define bin size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using smart binning.\n"
     ]
    }
   ],
   "source": [
    "binsize    = 20 # ns\n",
    "n_bins_old = 5000 # in 1ns bins\n",
    "\n",
    "smart_binning = True # enables quadratic binning of time profiles\n",
    "\n",
    "if not smart_binning:\n",
    "    n_bins_new = 5000/binsize\n",
    "    time_bins  = np.linspace(0, n_bins_old, (n_bins_new+1))\n",
    "    print 'Using binsize of', binsize, 'ns'\n",
    "else:\n",
    "    n_bins_new = 30\n",
    "    print 'Using smart binning.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in truth and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_binned          = {}\n",
    "data_binned[\"truth\"] = {}\n",
    "data_binned[\"data\"]  = {}\n",
    "data_binned[\"index\"] = {}\n",
    "data_binned[\"bins\"]  = {}\n",
    "\n",
    "### Read in truth ###\n",
    "for pocam in pocams:\n",
    "    par_t = param_string(t_abs, t_sca, t_dome, t_p0, t_p1, t_Nph, t_N, pocam)\n",
    "    data_tmp_1ns_binned = np.load(os.path.expanduser('%s/TRUTH_%s.npy'%(direc, par_t)))\n",
    "    data_binned[\"truth\"][pocam] = copy.deepcopy(event)\n",
    "    data_binned[\"index\"][pocam] = copy.deepcopy(event)\n",
    "    data_binned[\"bins\"][pocam]  = copy.deepcopy(event)\n",
    "    \n",
    "    for key in all_dom_keys:  #go through all DOMs\n",
    "        d = data_tmp_1ns_binned.item().get(key)\n",
    "        \n",
    "        if not smart_binning:\n",
    "            # re-sum bins to get new bins of size <binsize>\n",
    "            data_tmp_binned = [sum(d[i*binsize : (i+1)*binsize]) for i in range(n_bins_new)]\n",
    "        else:\n",
    "            # get POCAM coordinates\n",
    "            p   = pocam.split('-')\n",
    "            pk  = OMKey(int(p[0]), int(p[1]), 0)\n",
    "            p_x = geo.omgeo[pk].position.x\n",
    "            p_y = geo.omgeo[pk].position.y\n",
    "            p_z = geo.omgeo[pk].position.z\n",
    "\n",
    "            # get DOM coordinates\n",
    "            d_x = geo.omgeo[key].position.x\n",
    "            d_y = geo.omgeo[key].position.y\n",
    "            d_z = geo.omgeo[key].position.z\n",
    "\n",
    "            # get distance POCAM/DOM\n",
    "            distance = np.sqrt((d_x-p_x)**2 + (d_y-p_y)**2 + (d_z-p_z)**2)\n",
    "            \n",
    "            # get geometrical min time of photons\n",
    "            c_ice = 3e8 / 1.32\n",
    "            t_geo = int(np.floor(distance / c_ice * 1e9)) # [ns]\n",
    "            idx   = next((i for i, x in enumerate(d) if x), None)\n",
    "            data_binned[\"index\"][pocam][key] = t_geo\n",
    "            \n",
    "            # get new smart bins\n",
    "            smart_exp  = np.log(n_bins_old - t_geo) / np.log(n_bins_new)\n",
    "            smart_bins = [i**smart_exp for i in range(n_bins_new + 1)]\n",
    "            smart_bins = np.array([t_geo + int(round(i)) for i in smart_bins])\n",
    "            data_binned[\"bins\"][pocam][key] = smart_bins\n",
    "            \n",
    "            # re-sum data with smart bins\n",
    "            data_tmp_binned = [sum(d[smart_bins[i]:smart_bins[i+1]]) for i in range(len(smart_bins) - 1)]\n",
    "            \n",
    "        # eventually append to final dict\n",
    "        data_binned[\"truth\"][pocam][key]  = np.array(data_tmp_binned) / float(t_N / scan_N) # average to data size\n",
    "\n",
    "print \"Truth finished. Starting data...\\n\"\n",
    "    \n",
    "### Read in data ###\n",
    "for _abs in scan_abs:\n",
    "    for sca in scan_sca:\n",
    "        for dome in scan_dome:\n",
    "            for p0 in scan_p0:\n",
    "                for p1 in scan_p1:\n",
    "                    \n",
    "                    tag=\"{sca}_{dome}_{p0}_{p1}_{_abs}\".format(sca=sca, dome=dome, p0=p0, p1=p1, _abs=_abs)\n",
    "                    data_binned[\"data\"][tag] = {}\n",
    "                         \n",
    "                    for pocam in pocams: \n",
    "                        par = param_string(_abs, sca, dome, p0, p1, scan_Nph, scan_N, pocam)\n",
    "                        data_tmp_1ns_binned = np.load(os.path.expanduser('%s/%s.npy'%(direc, par)))\n",
    "                        data_binned[\"data\"][tag][pocam] = copy.deepcopy(event)\n",
    "\n",
    "                        for key in all_dom_keys:  # go through all DOMs\n",
    "                            d = data_tmp_1ns_binned.item().get(key)\n",
    "                            if not smart_binning:\n",
    "                                # re-sum bins to new binsize\n",
    "                                data_tmp_binned = [sum(d[i*binsize : (i+1)*binsize]) for i in range(n_bins_new)]\n",
    "                            else:\n",
    "                                smart_bins = data_binned[\"bins\"][pocam][key]\n",
    "                                t_geo      = data_binned[\"index\"][pocam][key]\n",
    "                                # re-sum with new smart bins\n",
    "                                data_tmp_binned = [sum(d[smart_bins[i]:smart_bins[i+1]]) for i in range(len(smart_bins) - 1)]\n",
    "                            \n",
    "                            data_binned[\"data\"][tag][pocam][key]  = data_tmp_binned\n",
    "                                \n",
    "                    print \"Parameter combo:\", _abs, sca, dome, p0, p1, \"done!\"\n",
    "print \"FINISHED!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = OMKey(84,60,0)\n",
    "pk = '90-100'\n",
    "\n",
    "if smart_binning:\n",
    "    tb = data_binned['bins'][pk][ok]\n",
    "    time_bins = (tb[1:] + tb[:-1]) / 2\n",
    "    \n",
    "else:\n",
    "    time_bins = time_bins[:-1]\n",
    "\n",
    "# plot expectation\n",
    "plt.figure()\n",
    "y_hist = data_binned[\"truth\"][pk][ok]\n",
    "plt.step(time_bins, y_hist, where='mid', alpha=0.9)\n",
    "plt.fill_between(time_bins, y_hist, step=\"mid\", alpha=0.3, label='truth')\n",
    "print('Expectation sum: %i' %sum(y_hist))\n",
    "\n",
    "# compare with\n",
    "params = \"{sca}_{dome}_{p0}_{p1}_{_abs}\".format(sca=1.00, dome=1.014, p0=-1.00, p1=0.086, _abs=1.00)\n",
    "y_hist = data_binned[\"data\"][params][pk][ok]\n",
    "plt.step(time_bins, y_hist, where='mid', alpha=0.9)\n",
    "plt.fill_between(time_bins, y_hist, step=\"mid\", alpha=0.3, label=\"example data\")\n",
    "\n",
    "# format\n",
    "plt.xlim(-100,4000)\n",
    "plt.ylim(0,None)\n",
    "plt.xlabel('Time [ns]', fontsize=14)\n",
    "plt.ylabel('Number of hits', fontsize=14)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('example_comp.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the LLH-landsape\n",
    "For the likelihhod we combine the likelihoods of individual DOMs of all POCAM flashes. To keep it managable and keep the individual count statistic high, we only look at DOMs that are within a 100m radius around the corresponing POCAM. \n",
    "<br>\n",
    "The likelihood is then:\n",
    "$$\n",
    "\\mathcal{L} = \\prod_{\\text{POCAMs}} \\, \\prod_{\\text{DOMs} < 100m} \\, \\prod_{\\text{histogram bins}} \\, p(d_i, t_i)\n",
    "$$\n",
    "and the log llh:\n",
    "$$\n",
    "-2 ln \\mathcal{L} = -2 \\sum_{\\text{POCAMs}} \\, \\sum_{\\text{DOMs}} \\, \\sum_{\\text{bins}} \\, ln( p(d_i, t_i))\n",
    "$$\n",
    "If $t_i$ > 20:\n",
    "$$\n",
    "p(d_i,t_i) = Normal(d_i,t_i) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\, e^{-\\frac{(d_i-t_i)^2}{2 \\sigma^2}} \\qquad \\text{where} \\quad \\sigma^2 = \\sqrt{t_i}^2 = t_i\n",
    "$$\n",
    "and if $t_i$ < 20:\n",
    "$$\n",
    "        p(d_i,t_i) = Poisson(d_i,t_i) = \\frac{t_i^{d_i}}{d_i !} \\, e^{-t_i} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 20 # number of datapoints taken after first hits\n",
    "\n",
    "if smart_binning:\n",
    "    n_data = n_bins_new"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d = {}\n",
    "np.save('./llhdict_sca-domeff.npy', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import llh_main\n",
    "reload(llh_main)\n",
    "\n",
    "# # check if bin/pts combination has been calculated before\n",
    "# llhfile = './llhdict_sca-domeff.npy'\n",
    "# llhdict = np.load(llhfile).item()\n",
    "# profilekey = '%i_%i' %(binsize, n_data)\n",
    "\n",
    "# if profilekey in llhdict:\n",
    "    \n",
    "#     print('LLH profile calculated before, using archived data.')\n",
    "#     llh_array = llhdict[profilekey]\n",
    "\n",
    "# else:\n",
    "\n",
    "#     print('New LLH profile, calculating...')\n",
    "    \n",
    "# Define empty llh-landscape:\n",
    "llh_array=np.zeros((len(scan_dome), len(scan_p1), len(scan_p0)))\n",
    "\n",
    "combo_cntr = 1\n",
    "for sca_i, sca in enumerate(scan_sca):\n",
    "\n",
    "    for dome_i, dome in enumerate(scan_dome):\n",
    "\n",
    "        for p_i, p0 in enumerate(scan_p0):\n",
    "\n",
    "            for p_j, p1 in enumerate(scan_p1):\n",
    "\n",
    "                for _abs in scan_abs:\n",
    "\n",
    "                    counter = 0\n",
    "\n",
    "                    # list of llh for pocams\n",
    "                    llh_pocam=[]\n",
    "\n",
    "                    for k,pocam in enumerate(pocams):\n",
    "\n",
    "                        # list of llh for doms\n",
    "                        llh_doms=[]\n",
    "\n",
    "                        # Get POCAM coordinates:\n",
    "                        pocam_key=pocam_keys[k]\n",
    "                        p_x=geo.omgeo[pocam_key].position.x\n",
    "                        p_y=geo.omgeo[pocam_key].position.y\n",
    "                        p_z=geo.omgeo[pocam_key].position.z\n",
    "\n",
    "                        hit_doms = 0\n",
    "                        for key in all_dom_keys:\n",
    "                            if key.pmt==0: # ignore upgrade receivers\n",
    "\n",
    "                                # list of llh for datapoints\n",
    "                                llh_datapoints=[]\n",
    "\n",
    "                                # Get DOM coordinates:\n",
    "                                d_x=geo.omgeo[key].position.x\n",
    "                                d_y=geo.omgeo[key].position.y\n",
    "                                d_z=geo.omgeo[key].position.z\n",
    "\n",
    "                                distance= np.sqrt((d_x-p_x)**2 + (d_y-p_y)**2 + (d_z-p_z)**2)\n",
    "\n",
    "                                # get data\n",
    "                                par      = \"{sca}_{dome}_{p0}_{p1}_{_abs}\".format(sca=sca,dome=dome,\n",
    "                                                                                  p0=p0,p1=p1,_abs=_abs)\n",
    "                                truth    = np.array(data_binned[\"truth\"][pocam][key])\n",
    "                                data_sim = np.array(data_binned[\"data\"][par][pocam][key])\n",
    "\n",
    "                                # print '%s %s \\t %.1f \\t %i %i' %(pocam, key, distance, sum(truth), sum(data_sim))\n",
    "\n",
    "                                min_q     = 0\n",
    "                                min_q_bin = 10\n",
    "\n",
    "                                if sum(truth) > min_q and sum(data_sim) > min_q: \n",
    "\n",
    "                                    #print '%s %s \\t %.1f \\t %i' %(pocam, key, distance,sum(truth))\n",
    "                                    hit_doms += 1\n",
    "\n",
    "                                    if not smart_binning:\n",
    "\n",
    "                                        index    = next((i for i, x in enumerate(truth) if x), None) # first non-zero\n",
    "                                        truth    = truth[index:index+n_data]\n",
    "                                        data_sim = data_sim[index:index+n_data]\n",
    "\n",
    "                                    for i in range(n_data):\n",
    "                                        t = float(truth[i])\n",
    "                                        d = float(data_sim[i])\n",
    "\n",
    "                                        # remove non-hit doms (and apply some minimum bin value)\n",
    "                                        if t > 0 and t > min_q_bin:\n",
    "\n",
    "                                            # get global llh definition\n",
    "                                            llh = llh_main.llh_grid(d,t)\n",
    "                                            # append it \n",
    "                                            llh_datapoints.append(llh)\n",
    "\n",
    "\n",
    "                                    # Append llh sum from DOM:\n",
    "                                    counter += 1\n",
    "                                    llh_doms.append(sum(llh_datapoints)) \n",
    "\n",
    "                        # Append llh sum from POCAM:\n",
    "                        llh_pocam.append(sum(llh_doms) / float(hit_doms)) \n",
    "\n",
    "                    # Putting the final llh into the landscape array:\n",
    "                    llh_array[dome_i, p_j, p_i] = sum(llh_pocam) / float(len(pocams))\n",
    "                    print('%i / %i combinations done.' %(combo_cntr, int(len(scan_sca) * len(scan_dome))))\n",
    "                    combo_cntr += 1\n",
    "    \n",
    "#     # store new profile \n",
    "#     llhdict[profilekey] = llh_array\n",
    "#     np.save(llhfile, llhdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# likelihood profile\n",
    "x  = scan_dome\n",
    "y  = scan_sca\n",
    "z  = llh_array\n",
    "dx = np.abs((np.unique(x)[1:] - np.unique(x)[:-1])/2)[0]\n",
    "dy = np.abs((np.unique(y)[1:] - np.unique(y)[:-1])/2)[0]\n",
    "xx = np.unique(np.append(x - dx, sorted(x + dx)[-1]))\n",
    "yy = np.unique(np.append(y - dy, sorted(y + dy)[-1]))\n",
    "im = ax.pcolormesh(xx, yy, z)\n",
    "\n",
    "# colorbar\n",
    "cbar = plt.colorbar(im, orientation='vertical',fraction=0.045,ax=ax)\n",
    "cbar.set_label(r'$-2 \\, \\frac{ln \\, \\mathcal{L}}{N}$',fontsize=18)\n",
    "\n",
    "# truth\n",
    "tx = t_dome\n",
    "ty = t_sca\n",
    "plt.scatter(tx, ty, color='r',s=150,label='MC truth',marker='*')\n",
    "\n",
    "# llh min\n",
    "index_1,index_2=np.where(llh_array==np.amin(llh_array))\n",
    "l_sca = scan_sca[int(index_1)]\n",
    "l_dome = scan_dome[int(index_2)]\n",
    "print \"LLH minimum at sca, domeff:\", (l_sca, l_dome)\n",
    "print \"Truth at                  :\", (t_sca[0], t_dome[0])\n",
    "plt.scatter(l_dome, l_sca, color='cyan',s=250,label='LLH min',marker='*')\n",
    "\n",
    "### llh intervals ###\n",
    "# index_s1,index_s2=np.where(llh_array<np.amin(llh_array)+2.7)\n",
    "# plt.scatter(index_s2,index_s1,label=\"$< llh_{min} + 2.7$\",c='yellow',s=100,alpha=0.8)\n",
    "\n",
    "# formatting\n",
    "ax.set_title(\"Grid search with {_bin} ns bins / {pts} pts\".format(_bin=binsize, pts=n_data),fontsize=14)\n",
    "ax.set_xlabel('Relative DOM Efficiency')\n",
    "ax.set_ylabel('Relative scattering coefficient')\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./grid-search-sca-domeff_binning-%ins-%ipts.pdf' %(binsize, n_data))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scan_dome\n",
    "y = scan_sca\n",
    "z = llh_array\n",
    "n = 100\n",
    "\n",
    "f    = interp2d(np.unique(x), np.unique(y), z, kind='cubic')\n",
    "xn   = np.linspace(x.min(), x.max(), n)\n",
    "yn   = np.linspace(y.min(), y.max(), n)\n",
    "zzz  = f(xn, yn)\n",
    "dxn  = np.abs((np.unique(xn)[1:] - np.unique(xn)[:-1])/2)[0]\n",
    "dyn  = np.abs((np.unique(yn)[1:] - np.unique(yn)[:-1])/2)[0]\n",
    "xxn  = np.unique(np.append(xn - dxn, sorted(xn + dxn)[-1]))\n",
    "yyn  = np.unique(np.append(yn - dyn, sorted(yn + dyn)[-1]))\n",
    "xxx, yyy = np.meshgrid(xxn, yyn)\n",
    "xxxm, yyym = np.meshgrid(xn, yn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# interpolated llh profile\n",
    "im = ax.pcolormesh(xxx,yyy,zzz)\n",
    "cb = plt.colorbar(im, orientation='vertical',\n",
    "                  fraction=0.05,ax=ax, label=r'$-2 \\, \\frac{ln \\, \\mathcal{L}}{N}$')\n",
    "\n",
    "# truth and min\n",
    "tx  = t_dome\n",
    "ty  = t_sca\n",
    "idx = np.where(zzz==np.amin(zzz))\n",
    "lx  = xxx[idx]\n",
    "ly  = yyy[idx]\n",
    "plt.scatter(tx, ty, color='r',s=150,label='MC truth',marker='*')\n",
    "plt.scatter(lx, ly, color='cyan',s=150,label='LLH min',marker='*')\n",
    "\n",
    "# contours\n",
    "zmin = np.min(zzz)    \n",
    "cs = ax.contour(xxxm, yyym, zzz, levels=zmin + 2.7 * np.array([1,2,3]), \n",
    "                colors=['white'], linestyles=['--'], label='Confidence level')\n",
    "fmt  = {}\n",
    "strs = [r'$1\\sigma$', r'$2\\sigma$', r'$3\\sigma$']\n",
    "for l, s in zip(cs.levels, strs):\n",
    "    fmt[l] = s\n",
    "ax.clabel(cs, cs.levels, inline=True, fmt=fmt)\n",
    "\n",
    "# legend\n",
    "hs, ls = ax.get_legend_handles_labels()\n",
    "h,_    = cs.legend_elements()\n",
    "plt.legend(hs+h, ls+['Confidence level'])\n",
    "plt.title(\"Grid search with {_bin} ns bins / {pts} pts\".format(_bin=binsize, pts=n_data),fontsize=14)\n",
    "plt.xlabel('Relative DOM Efficiency')\n",
    "plt.ylabel('Relative scattering coefficient')\n",
    "plt.savefig('./grid-search-sca-domeff_binning-%ins-%ipts_interp_chi2-mod+pocam-unc.pdf' %(binsize, n_data))\n",
    "plt.show()\n",
    "\n",
    "# find and print confidene levels\n",
    "idx_sig  = np.where(zzz <= np.min(zzz) + 2.7)\n",
    "\n",
    "lx_sig_n = np.min(xxx[idx_sig])\n",
    "lx_sig_p = np.max(xxx[idx_sig])\n",
    "\n",
    "ly_sig_n = np.min(yyy[idx_sig])\n",
    "ly_sig_p = np.max(yyy[idx_sig])\n",
    "print 'Truth at (domeff, sca):', (tx[0], ty[0])\n",
    "print 'LLH min at            :', (round(lx[0], 3), round(ly[0], 3))\n",
    "print 'Domeff +- 1sig        :', (round(tx[0] - lx_sig_n, 3), round(lx_sig_p -  tx[0], 3))\n",
    "print 'Scattering +- 1sig    :', (round(ty[0] - ly_sig_n, 3), round(ly_sig_p - ty[0], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
